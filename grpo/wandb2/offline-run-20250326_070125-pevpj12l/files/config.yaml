wandb_version: 1

_wandb:
  desc: null
  value:
    python_version: 3.9.21
    cli_version: 0.19.6
    framework: huggingface
    huggingface_version: 4.47.1
    is_jupyter_run: false
    is_kaggle_kernel: false
    start_time: 1742972485
    t:
      1:
      - 1
      - 5
      - 11
      - 41
      - 49
      - 51
      - 53
      - 55
      - 71
      - 98
      - 105
      3:
      - 4
      - 13
      - 16
      - 23
      - 42
      - 55
      4: 3.9.21
      5: 0.19.6
      6: 4.47.1
      8:
      - 5
      13: linux-x86_64
policy_model:
  desc: null
  value: "PolicyModel(\n  (loss_fn): MSELoss()\n  (model): PeftModel(\n    (base_model):\
    \ LoraModel(\n      (model): Qwen2ForCausalLM(\n        (model): Qwen2Model(\n\
    \          (embed_tokens): Embedding(151936, 1536)\n          (layers): ModuleList(\n\
    \            (0-27): 28 x Qwen2DecoderLayer(\n              (self_attn): Qwen2SdpaAttention(\n\
    \                (q_proj): lora.Linear(\n                  (base_layer): Linear(in_features=1536,\
    \ out_features=1536, bias=True)\n                  (lora_dropout): ModuleDict(\n\
    \                    (default): Dropout(p=0.1, inplace=False)\n              \
    \    )\n                  (lora_A): ModuleDict(\n                    (default):\
    \ Linear(in_features=1536, out_features=1, bias=False)\n                  )\n\
    \                  (lora_B): ModuleDict(\n                    (default): Linear(in_features=1,\
    \ out_features=1536, bias=False)\n                  )\n                  (lora_embedding_A):\
    \ ParameterDict()\n                  (lora_embedding_B): ParameterDict()\n   \
    \               (lora_magnitude_vector): ModuleDict()\n                )\n   \
    \             (k_proj): Linear(in_features=1536, out_features=256, bias=True)\n\
    \                (v_proj): lora.Linear(\n                  (base_layer): Linear(in_features=1536,\
    \ out_features=256, bias=True)\n                  (lora_dropout): ModuleDict(\n\
    \                    (default): Dropout(p=0.1, inplace=False)\n              \
    \    )\n                  (lora_A): ModuleDict(\n                    (default):\
    \ Linear(in_features=1536, out_features=1, bias=False)\n                  )\n\
    \                  (lora_B): ModuleDict(\n                    (default): Linear(in_features=1,\
    \ out_features=256, bias=False)\n                  )\n                  (lora_embedding_A):\
    \ ParameterDict()\n                  (lora_embedding_B): ParameterDict()\n   \
    \               (lora_magnitude_vector): ModuleDict()\n                )\n   \
    \             (o_proj): Linear(in_features=1536, out_features=1536, bias=False)\n\
    \                (rotary_emb): Qwen2RotaryEmbedding()\n              )\n     \
    \         (mlp): Qwen2MLP(\n                (gate_proj): Linear(in_features=1536,\
    \ out_features=8960, bias=False)\n                (up_proj): Linear(in_features=1536,\
    \ out_features=8960, bias=False)\n                (down_proj): Linear(in_features=8960,\
    \ out_features=1536, bias=False)\n                (act_fn): SiLU()\n         \
    \     )\n              (input_layernorm): Qwen2RMSNorm((1536,), eps=1e-06)\n \
    \             (post_attention_layernorm): Qwen2RMSNorm((1536,), eps=1e-06)\n \
    \           )\n          )\n          (norm): Qwen2RMSNorm((1536,), eps=1e-06)\n\
    \          (rotary_emb): Qwen2RotaryEmbedding()\n        )\n        (lm_head):\
    \ Linear(in_features=1536, out_features=151936, bias=False)\n      )\n    )\n\
    \  )\n)"
reward:
  desc: null
  value:
  - RLHF.grpo.gms8k_reward.format_reward
  - RLHF.grpo.gms8k_reward.correctness_reward
max_answer_seq_len:
  desc: null
  value: 512
n_updates_per_iteration:
  desc: null
  value: 5
clip:
  desc: null
  value: 0.2
lr:
  desc: null
  value: 0.001
save_steps:
  desc: null
  value: 60
eval_steps:
  desc: null
  value: 60
gamma:
  desc: null
  value: 0.95
epoch:
  desc: null
  value: 3
kl_ctl:
  desc: null
  value: 0.1
clip_reward_value:
  desc: null
  value: 5
batch_size:
  desc: null
  value: 8
reward_mode:
  desc: null
  value: rule
lam:
  desc: null
  value: 0.9
cliprange:
  desc: null
  value: 0.05
cliprange_value:
  desc: null
  value: 0.05
