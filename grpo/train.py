import argparse
import re
import time
import torch
import torch.nn as nn
from torch.utils.data import  DataLoader,Dataset

from transformers import get_scheduler,AutoTokenizer,PreTrainedModel,AutoModel,AutoModelForCausalLM, BertPreTrainedModel,Qwen2PreTrainedModel,Qwen2Model,DataCollatorWithPadding,BitsAndBytesConfig
from peft import LoraConfig, get_peft_model

from datasets import load_dataset
import wandb
from pathlib import Path

from copy import deepcopy
from dataclasses import dataclass
import inspect
import json
import os
from typing import Dict, Iterator, List, Optional, Sequence, Union, Iterable
import torch
import torch.nn as nn
from torch.nn import functional as F
import math
import gc
from datasets import dataset_dict, load_from_disk

from tqdm import tqdm, trange
from transformers import (
    CONFIG_MAPPING,
    MODEL_MAPPING,
    AutoConfig,
    AutoModelForCausalLM,
    AutoTokenizer,
    SchedulerType,
    default_data_collator,
    get_scheduler,
    LlamaForCausalLM,
    GPT2Tokenizer,
    GPT2Model,
    Qwen2Config
)


import sys
sys.path.append("/home/wsy/NLP/RL")

from RLHF.policy.policy import PolicyModel
from RLHF.policy.value import ValueModel
from RLHF.reward.rm import RewardModel
from RLHF.grpo.gms8k_reward import format_reward,correctness_reward


class GRPOTrainer():
    def __init__(self,args):
        pretrain_path=args.pretrain_path
        train_path=args.train_path
        test_path=args.test_path
        self.device=args.device
        self.use_wandb=args.use_wandb


        self.max_answer_seq_len=512
        self.n_updates_per_iteration = 5
        self.clip = 0.2 # As recommended by the paper
        self.lr=0.001
        self.save_freq=10
        self.gamma = 0.95 
        self.epoch=1
        self.kl_ctl=0.1
        self.clip_reward_value = 5
        self.batch_size=2
        self.num_generation=2
        self.reward_mode="model" if args.reward_model else "rule"  #{"model","rule"}
        self.lam = 0.9
        self.cliprange = 0.05
        self.cliprange_value = 0.05
        self.best_reward = float('-inf')
        self.warmup_ratio=0.0
        self.epsilon = 0.00001
        self.beta=0.01
        

        self.output_dir = Path(args.output_dir)
        self.output_dir.mkdir(exist_ok=True, parents=True)


        train_data=load_dataset("parquet", data_files=train_path,split='train',streaming=True).shuffle(seed=42).take(8)
        test_data=load_dataset("parquet", data_files=test_path,split='train',streaming=True).shuffle(seed=42).take(4)


        
        self.tokenizer=AutoTokenizer.from_pretrained(pretrain_path)
        self.tokenizer.padding_side='left'

        bnb_config = BitsAndBytesConfig(
            load_in_8bit=False,  # ä¸ä½¿ç”¨8bit
            load_in_4bit=False,  # ä¸ä½¿ç”¨4bit
            load_in_2bit=True,   # å¯ç”¨ 2-bit é‡åŒ–
            bnb_2bit_compute_dtype=torch.float16,  # è®¡ç®—æ—¶ä½¿ç”¨ float16
            bnb_2bit_quant_type="nf2"  # `nf2` é‡åŒ–æ ¼å¼ï¼Œé€‚ç”¨äº LLM
        )
        # bnb_config=None


        # è®¾ç½® LoRA é…ç½®
        lora_config = LoraConfig(
            r=1,  # Rankï¼Œè¶Šå¤§è¡¨ç¤º LoRA å±‚è¶Šå¤§ï¼Œæ¶ˆè€—æ˜¾å­˜æ›´å¤š
            lora_alpha=8,  # LoRA scaling factor
            lora_dropout=0.1,  # Dropout é˜²æ­¢è¿‡æ‹Ÿåˆ
            target_modules=["q_proj", "v_proj"],  # åªè®­ç»ƒæ³¨æ„åŠ›å±‚
            bias="none",
            # task_type="CAUSAL_LM"  # é€‚ç”¨äºè‡ªå›å½’ï¼ˆdecoder-onlyï¼‰æ¨¡å‹ï¼Œå¦‚ Qwen
        )


        self.policy_model=PolicyModel(pretrain_path,lora_config,bnb_config=bnb_config).to(self.device)
        # TODO è®°å¾—è¦æŠŠè¿™é‡Œæ”¹å›æ¥
        self.ref_model=deepcopy(self.policy_model).to(self.device)
        # self.ref_model=self.policy_model




        if args.reward_model:
            self.reward=RewardModel(args.reward_model,lora_config,bnb_config=bnb_config).to(self.device)
        else:
            self.reward=args.reward_fn


        self.train_dataset=data_prepare(self.tokenizer,train_data,self.device)
        self.test_dataset=data_prepare(self.tokenizer,test_data,self.device)

        self.train_dataloader=DataLoader(dataset=CustomDataset(self.train_dataset),shuffle=True,batch_size=self.batch_size)
        self.test_dataloader=DataLoader(dataset=CustomDataset(self.test_dataset),shuffle=False,batch_size=self.batch_size)

        
        if self.tokenizer.unk_token is None:
            self.tokenizer.unk_token = self.tokenizer.pad_token
        


        # åˆå§‹åŒ–wandb
        # if args.use_wandb:
        wandb.init(
            project='rlhf-grpo',
            name=f"grpo-{time.strftime('%Y%m%d-%H%M%S')}",
            config={
                "policy_model": self.policy_model,
                "reward": self.reward,
                "max_answer_seq_len": self.max_answer_seq_len,
                "n_updates_per_iteration": self.n_updates_per_iteration,
                "clip": self.clip,
                "lr": self.lr,
                "save_freq": self.save_freq,
                "gamma": self.gamma,
                "epoch": self.epoch,
                "kl_ctl": self.kl_ctl,
                "clip_reward_value": self.clip_reward_value,
                "batch_size": self.batch_size,
                "reward_mode": self.reward_mode,
                "lam": self.lam,
                "cliprange": self.cliprange,
                "cliprange_value": self.cliprange_value
            }
        )



        max_train_steps = self.epoch * len(self.train_dataloader)
        warm_steps = int(self.warmup_ratio * max_train_steps)


        no_decay = ["bias", "LayerNorm.weight"]
        self.policy_optimizer_grouped_parameters = [
            {
                "params": [p for n, p in self.policy_model.named_parameters() if not any(nd in n for nd in no_decay)],
                "weight_decay": 0.01,
            },
            {
                "params": [p for n, p in self.policy_model.named_parameters() if any(nd in n for nd in no_decay)],
                "weight_decay": 0.0,
            },
        ]
        self.policy_optimizer = torch.optim.AdamW(self.policy_optimizer_grouped_parameters, lr=self.lr)

        
        self.policy_lr_scheduler = get_scheduler(
            name='linear',
            optimizer=self.policy_optimizer,
            num_warmup_steps=warm_steps,
            num_training_steps=max_train_steps,
        )

        self.policy_optimizer=torch.optim.Adam(self.policy_model.parameters(),lr=self.lr)


    def learn(self):
        """
        epochè½®å­¦ä¹ 
        grpo on policy
        """
        print("==============grpo learn==============")
        self.policy_model.train()
        for i in range(self.epoch):

            pbar = tqdm(
                self.train_dataloader,
                desc=f"Train epoch [{i + 1}/{self.epoch}]",
            )

            
            policy_loss_sum, reward_sum = 0, 0
            batch_count = 0
            self.policy_optimizer.zero_grad()

            for batch in pbar:
                batch=self.stack_batches(batch)
                """
                grpoæ˜¯on policyï¼Œæ‰€ä»¥æ¯æ¬¡éƒ½è¦é‡‡æ ·å½“å‰æ¨¡å‹çš„completion
                """
                prepared_inputs=self.prepare_inputs(batch)
                rwd_score=torch.tensor(prepared_inputs["rewards"])
                policy_loss=self.step(prepared_inputs)
                self.policy_optimizer.step()
                self.policy_lr_scheduler.step()
                self.policy_optimizer.zero_grad()
                policy_loss_sum += policy_loss.item()

                print(f"rwd_score:{rwd_score}")
                reward_sum += rwd_score.sum().item()
                batch_count += 1
                
                # æ›´æ–°è¿›åº¦æ¡ä¿¡æ¯
                pbar.set_postfix({
                    'policy_loss': policy_loss.item(),
                    'reward': rwd_score.mean().item()
                })
                
                # è®°å½•åˆ°wandb
                wandb.log({
                    "policy_loss": policy_loss.item(),
                    "reward": rwd_score.mean().item(),
                    "learning_rate": self.policy_lr_scheduler.get_last_lr()[0],
                })
            avg_policy_loss = policy_loss_sum / max(1, batch_count)
            avg_reward = reward_sum / max(1, batch_count)

            print(f"Epoch {i+1}/{self.epoch} - Avg Policy Loss: {avg_policy_loss:.4f}, Avg Reward: {avg_reward:.4f}")
            
            # è®°å½•åˆ°wandb
            if self.use_wandb:
                wandb.log({
                    "epoch": i+1,
                    "avg_policy_loss": avg_policy_loss,
                    "avg_reward": avg_reward,
                })

            # åœ¨æµ‹è¯•é›†ä¸Šè¯„ä¼°
            test_reward = self.evaluate()
            
            # ä¿å­˜æ¨¡å‹
            if (i+1) % self.save_freq == 0 or i == self.epoch - 1:
                self.save_checkpoint(f"checkpoint-epoch-{i+1}")
            
            # ä¿å­˜æœ€ä½³æ¨¡å‹
            if test_reward > self.best_reward:
                self.best_reward = test_reward
                self.save_checkpoint("best_model")
                print(f"New best model with reward: {test_reward:.4f}")

            
        wandb.finish()
        
    def prepare_inputs(self,batch):
        """
        å¯¹on policyç”Ÿæˆçš„batché‡‡æ · + æ‰“åˆ† + è®¡ç®—ç›¸å¯¹ä¼˜åŠ¿
        """
        self.eval()
        for k, v in batch.items():
            if isinstance(v, torch.Tensor):
                batch[k] = v.to(self.device)
                
        pad_token_id = self.tokenizer.pad_token_id
        input_ids=batch["input_ids"]
        prompt=self.tokenizer.batch_decode(input_ids, skip_special_tokens=True)[0]

        with torch.no_grad():
            
            # sanitize_input_ids(self.tokenizer,batch["input_ids"],self.tokenizer.vocab_size)

            seq = self.policy_model.model.generate(
                batch["input_ids"],
                attention_mask=batch["attention_mask"],
                max_length=self.max_answer_seq_len,
                pad_token_id=self.tokenizer.pad_token_id,
                )

            seq_mask = seq.not_equal(pad_token_id).long()
            completions =self.get_completion(seq, input_ids, self.tokenizer)
            
        
            outputs=self.policy_model(seq, attention_mask=seq_mask)
            outputs_ref=self.ref_model(seq, attention_mask=seq_mask)
            rwd_score=self.compute_reward_score(seq,attention_mask=seq_mask,completions=completions,labels=batch["labels"])
            print(f"rwd_score:{rwd_score}")
            adv=self.compute_adv(rwd_score)
            print(f"adv:{adv}")
            

        logits = outputs.logits
        logits_ref = outputs_ref.logits

        prepared={
            'prompts': input_ids,
            'logprobs': gather_log_probs(logits[:, :-1, :], seq[:, 1:]),
            'ref_logprobs': gather_log_probs(logits_ref[:, :-1, :], seq[:,1:]),
            'rewards': rwd_score,
            'advantage':adv,
            'input_ids': seq,
            "attention_mask": seq_mask,
            "completions":completions,
        }


        return prepared


    def get_completion(self,seq,input_ids, tokenizer):
        """
        ä»ç”Ÿæˆçš„åºåˆ—ä¸­æå– completion éƒ¨åˆ†ã€‚

        Args:
            seq (torch.Tensor): ç”Ÿæˆçš„åºåˆ—ï¼Œå½¢çŠ¶ä¸º (batch_size, seq_len)ã€‚
            input_ids (torch.Tensor): è¾“å…¥çš„ prompt çš„ token IDsï¼Œå½¢çŠ¶ä¸º (batch_size, prompt_len)ã€‚
            tokenizer: ç”¨äºè§£ç  token IDs çš„ tokenizerã€‚

        Returns:
            list: åŒ…å« completion å­—ç¬¦ä¸²çš„åˆ—è¡¨ã€‚
        """
        print("=============get completions===========")
        completions = []
        completions_mask=[]
        batch_size = seq.size(0)
        print(batch_size)

        for i in range(batch_size):
            # è·å–å½“å‰æ ·æœ¬çš„ prompt é•¿åº¦
            prompt_len = input_ids[i].size(0)
            # æå– completion éƒ¨åˆ†çš„ token IDs
            completion_ids = seq[i, prompt_len:]

            # è§£ç  completion éƒ¨åˆ†
            completion = tokenizer.decode(completion_ids, skip_special_tokens=True)
            completions.append(completion)

        # print(completions)
        return completions


    def step(self, prepared_inputs):
        """
        å•æ­¥æ›´æ–°
        """
        # train the rlhf mode here
        ### process the old outputs
        prompts = prepared_inputs['prompts']
        log_probs = prepared_inputs['logprobs']
        ref_log_probs = prepared_inputs['ref_logprobs']
        reward_score = prepared_inputs['rewards']
        attention_mask = prepared_inputs['attention_mask']
        seq = prepared_inputs['input_ids']
        adv=prepared_inputs['advantage']

        start = prompts.size()[-1] - 1
        # #å› ä¸ºç¬¬ä¸€ä¸ªtokenæ˜¯è¾“å…¥
        action_mask = attention_mask[:, 1:]
        # ç¡®ä¿åªæœ‰ç”Ÿæˆéƒ¨åˆ†çš„æœ‰æ•ˆ token å‚ä¸è®­ç»ƒï¼Œå¿½ç•¥ padding éƒ¨åˆ†ã€‚
        ends = start +attention_mask[:, start+1:].sum(1)-1


        batch = {'input_ids': seq, "attention_mask": attention_mask}

        policy_prob = self.policy_model(**batch).logits
        
            
            

        policy_log_prob = gather_log_probs(policy_prob[:, :-1, :], seq[:, 1:])
        print("policy_log_prob.requires_grad:", policy_log_prob.requires_grad)
        policy_loss = self.compute_loss(policy_log_prob[:, start:],
                                        log_probs[:, start:], ref_log_probs[:, start:],
                                        action_mask[:, start:],adv)
        policy_loss.backward()
        # # self.policy_model.backward(policy_loss)
        self.policy_optimizer.step()
        self.policy_lr_scheduler.step()

        self.policy_optimizer.zero_grad()

        return policy_loss
    
    def compute_reward_score(self,seq,attention_mask,completions,labels):
        """
        æ ¹æ®reward modelæˆ–è€…reward functionè®¡ç®—rwd
        å¥–åŠ±æ˜¯sentence-levelçš„ï¼Œå¥–åŠ±æ˜¯æ ‡é‡å€¼ã€‚
        """
        print('========compute reward score=======')
        print(self.reward_mode)
        size=seq.shape[0]
        if self.reward=="model":

            total_rewards=self.reward(seq,attention_mask=attention_mask)
                            
        elif self.reward_mode=="rule":
            total_rewards=[0 for i in range(size)]
            for rwd_fn in self.reward:
                rewards=rwd_fn(completions,labels) 
                total_rewards = [r1 + r2 for r1, r2 in zip(total_rewards, rewards)]

        print(total_rewards)
        return total_rewards
    
        

    def compute_adv(self,rewards):
        """
        æ ¹æ®reward scoreè®¡ç®—advantage
        åŒ…å«è¿‡ç¨‹ç›‘ç£å¼ºåŒ–å­¦ä¹ +GRPO å’Œ ç»“æœç›‘ç£å¼ºåŒ–å­¦ä¹ +GRPO ä¸¤ç§æ–¹å¼
        Ai,t=(ri-mean(r))/std(r), tå¯¹åº”token-levelä¼˜åŠ¿ï¼Œå³ä¸€ä¸ªå¥å­ä¸­ï¼Œæ¯ä¸ªtokenå¯¹åº”çš„ä¼˜åŠ¿æ˜¯ä¸€æ ·çš„ã€‚è¿™ç§æ–¹å¼çš„å¥½å¤„åœ¨äºï¼Œä¼°è®¡éƒ½æ˜¯ä»çœŸå®çš„ç¯å¢ƒrewardè®¡ç®—å¾—æ¥ï¼Œè€Œä¸æ˜¯é€šè¿‡ä»·å€¼ä¼°è®¡è®¡ç®—è€Œå¾—ã€‚
        """
        
        rewards = torch.tensor(rewards, dtype = torch.float).to(self.device)
        A = (rewards - rewards.mean()) / (rewards.std() + self.epsilon)
        return A





    def compute_loss(self,log_probs,old_log_probs,ref_log_probs,mask,adv):
        """
        æ ¹æ®advantageå’Œklæ•£åº¦è®¡ç®—loss
        GRPO KLæ˜¯token-levelçš„
        GRPOå¹¶æ²¡æœ‰åœ¨å¥–åŠ±ä¸­æ·»åŠ KLæƒ©ç½šï¼Œè€Œæ˜¯é€šè¿‡ç›´æ¥å°†è®­ç»ƒç­–ç•¥å’Œå‚è€ƒç­–ç•¥ä¹‹é—´çš„KLæ•£åº¦æ·»åŠ åˆ°æŸå¤±å‡½æ•°ä¸­æ¥è¿›è¡Œæ­£åˆ™åŒ–,ä»è€Œé¿å…äº†ä½¿å¾—ğ´^ğ‘–,ğ‘¡çš„è®¡ç®—å˜å¾—å¤æ‚
        å½“å‰ç­–ç•¥å¦‚æœå’Œrefç­–ç•¥æ¥è¿‘ï¼Œåˆ™klæ¥è¿‘0ï¼Œlosså¯èƒ½æ˜¯è´Ÿæ•°
        """
        print(f"===========compute loss=========")
        len_oi=mask[:, ].sum(1)
        
        # kl
        kl=ref_log_probs.exp() / log_probs.exp()- (ref_log_probs - log_probs) - 1
        ratio=torch.exp(ref_log_probs - old_log_probs)
        adv=adv.unsqueeze(dim = 1)  # [a, b ,c] -> [[a], [b], [c]]
        loss1=ratio*adv
        loss2=reward_clip = torch.clamp(ratio, 1.0 - self.cliprange,
                                  1.0 + self.cliprange)*adv
        loss=(torch.minimum(loss1,loss2)-self.beta*kl)*mask
        loss=-(1/self.num_generation)*(1/len_oi.unsqueeze(dim = 1))*loss
        loss = loss.sum()

        return loss







    def evaluate(self):
        """
        è¯„ä¼°å½“å‰policyçš„æ•ˆæœ
        """
        print("==============Evaluating on test set==============")
        self.policy_model.eval()
        total_reward = 0
        generated_examples = []
        num_samples = min(5, len(self.test_dataloader))  # ä»…è®°å½•å°‘é‡æ ·æœ¬ç”¨äºå±•ç¤º
        
        with torch.no_grad():
            for idx, batch in enumerate(tqdm(self.test_dataloader, desc="Evaluating")):
                # å°†æ•°æ®ç§»åˆ°ç›¸åº”è®¾å¤‡
                for k, v in batch.items():
                    if isinstance(v, torch.Tensor):
                        batch[k] = v.to(self.device)
                
                input_ids = batch["input_ids"]
                prompts = self.tokenizer.batch_decode(input_ids, skip_special_tokens=True)
                
                try:
                    seq = self.policy_model.model.generate(
                        batch["input_ids"],
                        attention_mask=batch["attention_mask"],
                        max_length=self.max_answer_seq_len,
                        pad_token_id=self.tokenizer.pad_token_id,
                    )
                    
                    seq_mask = seq.not_equal(self.tokenizer.pad_token_id).long()
                    
                    # è®¡ç®—å¥–åŠ±åˆ†æ•°
                    reward = self.compute_reward_score(seq, attention_mask=seq_mask)
                    total_reward += reward.mean().item()
                    
                    # è§£ç ç”Ÿæˆçš„å›ç­”
                    generations = self.tokenizer.batch_decode(seq, skip_special_tokens=True)
                    
                    # ä¿å­˜ä¸€äº›æ ·æœ¬ç”¨äºå±•ç¤º
                    if idx < num_samples:
                        for p, g, r in zip(prompts, generations, reward):
                            generated_examples.append({
                                "prompt": p,
                                "generation": g,
                                "reward": r.item()
                            })
                
                except Exception as e:
                    print(f"Error during evaluation: {e}")
                    continue
        
        # è®¡ç®—å¹³å‡å¥–åŠ±
        avg_reward = total_reward / len(self.test_dataloader)
        print(f"Test set - Average reward: {avg_reward:.4f}")
        

        wandb.log({"test_reward": avg_reward})
        
        # åˆ›å»ºä¸€ä¸ªè¡¨æ ¼è®°å½•ç”Ÿæˆæ ·æœ¬
        if generated_examples:
            table = wandb.Table(columns=["prompt", "generation", "reward"])
            for example in generated_examples:
                table.add_data(example["prompt"], example["generation"], example["reward"])
            wandb.log({"generation_examples": table})
            wandb.log({"generation_examples": generated_examples})
        
        
        return avg_reward

    def save_checkpoint(self,checkpoint_name):
        """
        ä¿å­˜å½“å‰policy
        """
        checkpoint_dir = self.output_dir / checkpoint_name
        checkpoint_dir.mkdir(exist_ok=True, parents=True)
        
        # ä¿å­˜æ¨¡å‹
        self.policy_model.save_pretrained(checkpoint_dir / "policy_model")
        
        # ä¿å­˜tokenizer
        self.tokenizer.save_pretrained(checkpoint_dir)
        
        # ä¿å­˜è®­ç»ƒé…ç½®
        with open(checkpoint_dir / "training_args.json", 'w') as f:
            json.dump(vars(self.args), f, indent=2)
        
        print(f"Model checkpoint saved to {checkpoint_dir}")

    def eval(self):
        self.policy_model.eval()
        if self.reward_mode=="model":
            self.reward_model.eval()
        self.ref_model.eval()

    def stack_batches(self,batch):
        # # å°†æ•°æ®ç§»åˆ°ç›¸åº”è®¾å¤‡
        # for k, v in batch.items():
        #     if isinstance(v, torch.Tensor):
        #         batch[k] = v.to(self.device)
        stack_batch=batch
        for i in range(self.num_generation-1):
            stack_batch={key: torch.cat([stack_batch[key], batch[key]], dim=0) for key in batch}
        return stack_batch



def data_prepare(tokenizer,data_lst,device):
    
    question_lst=[data['prompt'][0]['content']for data in data_lst]


    gt_lst=[data["reward_model"]["ground_truth"]for data in data_lst]

    train_data = tokenizer.batch_encode_plus(question_lst, max_length=512, padding="longest", truncation=True,return_tensors='pt').to(device) 
    label_data = torch.tensor(gt_lst).to(device) 

    train_data["labels"] = label_data

    return train_data


class CustomDataset(Dataset):               
    def __init__(self, sample):
        super(CustomDataset, self).__init__()
        self.sample = sample

    def __getitem__(self, item):
        res = {k: v[item] for k, v in self.sample.items()}
        return res

    def __len__(self):
        return len(self.sample['input_ids'])



def gather_log_probs(logits, labels):
    """
    è·å¾—labelçš„å¯¹æ•°æ¦‚ç‡
    """
    log_probs = F.log_softmax(logits, dim=-1)
    log_probs_labels = log_probs.gather(dim=-1, index=labels.unsqueeze(-1))
    return log_probs_labels.squeeze(-1)






if __name__=="__main__":
    os.environ["WANDB_MODE"] = "offline"
    parser = argparse.ArgumentParser()

    

    

    # Models
    parser.add_argument("--pretrain_path", type=str, default='Qwen/Qwen2.5-0.5B-Instruct')
    # Dataset
    parser.add_argument("--train_path",default='/home/wsy/NLP/RL/RLHF/datatset/gsm8k/train.parquet')
    parser.add_argument("--test_path", default='/home/wsy/NLP/RL/RLHF/datatset/gsm8k/test.parquet')
    #wandb
    parser.add_argument("--use_wandb", default=True)
    #outputs
    parser.add_argument("--output_dir", default='/home/wsy/NLP/RL/RLHF/outputs/gsm8k/')
    parser.add_argument("--reward_model", default=None)


    args=parser.parse_args()

    device = torch.device("cuda" if torch.cuda.is_available() else "cpu")
    # device="cpu"
    print(f"device:{device}")

    args.device=device
    args.reward_fn=[format_reward,correctness_reward]

    grpoTrainer=GRPOTrainer(args)
    grpoTrainer.learn()