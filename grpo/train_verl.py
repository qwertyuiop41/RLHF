# æ¨¡ä»¿verlä¸­çš„GRPOå®ç°ï¼Œå°†klç›´æ¥åŠ åœ¨rewardä¸Šï¼Œåœ¨å¾—åˆ°advå’Œlossè¿›è¡Œè®­ç»ƒ
# æå‡ºçš„GRPOç®—æ³•ä¸­åº”è¯¥æ˜¯ç›´æ¥å°†klåŠ åœ¨lossä¸Š(é€šè¿‡rewardå’Œadvè·å¾—loss)


import argparse
from collections import defaultdict
import random
import re
import time
import numpy
import torch
import torch.nn as nn
from torch.utils.data import  DataLoader,Dataset

from transformers import get_scheduler,AutoTokenizer,PreTrainedModel,AutoModel,AutoModelForCausalLM, BertPreTrainedModel,Qwen2PreTrainedModel,Qwen2Model,DataCollatorWithPadding,BitsAndBytesConfig
from peft import LoraConfig, get_peft_model

from datasets import load_dataset
import wandb
from pathlib import Path

from copy import deepcopy
from dataclasses import dataclass
import inspect
import json
import os
from typing import Dict, Iterator, List, Optional, Sequence, Union, Iterable
import torch
import torch.nn as nn
from torch.nn import functional as F
import math
import gc
from datasets import dataset_dict, load_from_disk

from tqdm import tqdm, trange
from transformers import (
    CONFIG_MAPPING,
    MODEL_MAPPING,
    AutoConfig,
    AutoModelForCausalLM,
    AutoTokenizer,
    SchedulerType,
    default_data_collator,
    get_scheduler,
    LlamaForCausalLM,
    GPT2Tokenizer,
    GPT2Model,
    Qwen2Config
)


import sys
sys.path.append('./')


from policy.policy import PolicyModel
from policy.value import ValueModel
from reward.rm import RewardModel
from grpo.gms8k_reward import format_reward,correctness_reward
from grpo.kk import compute_score

class GRPOTrainer():
    def __init__(self,args):
        pretrain_path=args.pretrain_path
        train_path=args.train_path
        test_path=args.test_path
        self.device=args.device
        self.use_wandb=args.use_wandb


        self.max_answer_seq_len=1600
        self.lr=1e-6
        self.save_steps=200
        self.eval_steps=50
        self.gamma = 1.0  #åŸæœ¬0.95 verl 1.0
        self.epoch=2
        self.kl_ctl=0.001
        self.clip_reward_value = 1.0
        self.batch_size=1
        self.test_batch_size=16
        self.num_generation=6
        self.reward_mode="model" if args.reward_model else "rule"  #{"model","rule"}
        self.reward_fn="kk" #{"kk","gms8k"}
        self.lam = 1.0  #åŸæœ¬ 0.9 0.95 verl 1.0
        self.cliprange = 0.5
        self.best_reward = float('-inf')
        self.warmup_ratio=0.0
        self.epsilon = 1e-6   #åŸæœ¬0.00001   verl 1e-6
        self.beta=0.01
        

        self.output_dir = Path(args.output_dir)
        self.output_dir.mkdir(exist_ok=True, parents=True)


        train_data=load_dataset("parquet", data_files=train_path,split='train',streaming=True).shuffle(seed=42).take(400)
        test_data=load_dataset("parquet", data_files=test_path,split='train',streaming=True).shuffle(seed=42).take(40)


        
        self.tokenizer=AutoTokenizer.from_pretrained(pretrain_path)
        self.tokenizer.padding_side='left'

        # bnb_config = BitsAndBytesConfig(
        #     load_in_8bit=False,  # ä¸ä½¿ç”¨8bit
        #     load_in_4bit=False,  # ä¸ä½¿ç”¨4bit
        #     load_in_2bit=True,   # å¯ç”¨ 2-bit é‡åŒ–
        #     bnb_2bit_compute_dtype=torch.float16,  # è®¡ç®—æ—¶ä½¿ç”¨ float16
        #     bnb_2bit_quant_type="nf2"  # `nf2` é‡åŒ–æ ¼å¼ï¼Œé€‚ç”¨äº LLM
        # )
        bnb_config=None


        # è®¾ç½® LoRA é…ç½®
        lora_config = LoraConfig(
            r=4,  # Rankï¼Œè¶Šå¤§è¡¨ç¤º LoRA å±‚è¶Šå¤§ï¼Œæ¶ˆè€—æ˜¾å­˜æ›´å¤š
            lora_alpha=8,  # LoRA scaling factor
            lora_dropout=0.05,  # Dropout é˜²æ­¢è¿‡æ‹Ÿåˆ
            target_modules=["q_proj", "v_proj", "k_proj", "o_proj"],  # åªè®­ç»ƒæ³¨æ„åŠ›å±‚
            bias="none",
            # task_type="CAUSAL_LM"  # é€‚ç”¨äºè‡ªå›å½’ï¼ˆdecoder-onlyï¼‰æ¨¡å‹ï¼Œå¦‚ Qwen
        )


        self.policy_model=PolicyModel(pretrain_path,lora_config,bnb_config=bnb_config).to(self.device)

        self.ref_model=deepcopy(self.policy_model).to(self.device)
        # self.ref_model=self.policy_model




        if args.reward_model:
            self.reward=RewardModel(args.reward_model,lora_config,bnb_config=bnb_config).to(self.device)
        else:
            self.reward=args.reward_fn


        self.train_dataset=data_prepare(self.tokenizer,train_data,self.device)
        self.test_dataset=data_prepare(self.tokenizer,test_data,self.device)

        self.train_dataloader=DataLoader(dataset=CustomDataset(self.train_dataset),shuffle=True,batch_size=self.batch_size)
        self.test_dataloader=DataLoader(dataset=CustomDataset(self.test_dataset),shuffle=False,batch_size=self.test_batch_size)

        
        if self.tokenizer.unk_token is None:
            self.tokenizer.unk_token = self.tokenizer.pad_token
        


        # åˆå§‹åŒ–wandb
        # if args.use_wandb:
        wandb.init(
            project='rlhf-grpo-1.5b-4',
            name=f"grpo-{time.strftime('%Y%m%d-%H%M%S')}",
            dir="grpo",
            sync_tensorboard=True,
            config={
                "policy_model": args.pretrain_path,
                "lora_config": lora_config,
                "reward": self.reward,
                "max_answer_seq_len": self.max_answer_seq_len,
                "lr": self.lr,
                "save_steps": self.save_steps,
                "eval_steps": self.eval_steps,
                "gamma": self.gamma,
                "epoch": self.epoch,
                "kl_ctl": self.kl_ctl,
                "clip_reward_value": self.clip_reward_value,
                "batch_size": self.batch_size,
                "reward_mode": self.reward_mode,
                "lam": self.lam,
                "cliprange": self.cliprange,
                "num_generation":self.num_generation,
            }
        )



        max_train_steps = self.epoch * len(self.train_dataloader)
        warm_steps = int(self.warmup_ratio * max_train_steps)


        no_decay = ["bias", "LayerNorm.weight"]
        self.policy_optimizer_grouped_parameters = [
            {
                "params": [p for n, p in self.policy_model.named_parameters() if not any(nd in n for nd in no_decay)],
                "weight_decay": 0.01,
            },
            {
                "params": [p for n, p in self.policy_model.named_parameters() if any(nd in n for nd in no_decay)],
                "weight_decay": 0.0,
            },
        ]
        self.policy_optimizer = torch.optim.AdamW(self.policy_optimizer_grouped_parameters, lr=self.lr)

        
        self.policy_lr_scheduler = get_scheduler(
            name='linear',
            optimizer=self.policy_optimizer,
            num_warmup_steps=warm_steps,
            num_training_steps=max_train_steps,
        )

        self.policy_optimizer=torch.optim.Adam(self.policy_model.parameters(),lr=self.lr)


    def learn(self):
        """
        epochè½®å­¦ä¹ 
        grpo on policy
        """
        print("==============grpo learn==============")
        global_step=0
        self.policy_model.train()
        for i in range(self.epoch):

            pbar = tqdm(
                self.train_dataloader,
                desc=f"Train epoch [{i + 1}/{self.epoch}]",
            )

            
            policy_loss_sum, reward_sum = 0, 0
            batch_count = 0
            
            self.policy_optimizer.zero_grad()

            for batch in pbar:
                batch=self.stack_batches(batch)
                """
                grpoæ˜¯on policyï¼Œæ‰€ä»¥æ¯æ¬¡éƒ½è¦é‡‡æ ·å½“å‰æ¨¡å‹çš„completion
                """
                prepared_inputs=self.prepare_inputs(batch)
                rwd_score=prepared_inputs["rewards"].clone().detach()
                # rwd_score=torch.tensor(prepared_inputs["rewards"])
                policy_loss=self.step(prepared_inputs)
                self.policy_optimizer.step()
                self.policy_lr_scheduler.step()
                self.policy_optimizer.zero_grad()
                policy_loss_sum += policy_loss.item()

                reward_sum += rwd_score.sum().item()
                
                
                # æ›´æ–°è¿›åº¦æ¡ä¿¡æ¯
                pbar.set_postfix({
                    'policy_loss': policy_loss.item(),
                    'reward': rwd_score.float().sum().item()
                })

                seq_length=prepared_inputs["input_ids"].shape[1]
                
                # è®°å½•åˆ°wandb
                wandb.log({
                    "policy_loss": policy_loss.item(),
                    "reward": rwd_score.float().sum().item(),
                    "learning_rate": self.policy_lr_scheduler.get_last_lr()[0],
                    "seq_length": seq_length,
                })

                # # å®šæœŸä¿å­˜æ¨¡å‹
                # if global_step > 0 and global_step % self.save_steps == 0:
                #     self.save_checkpoint(f"checkpoint-steps-{global_step}")

                

                if global_step % self.eval_steps == 0:
                    # åœ¨æµ‹è¯•é›†ä¸Šè¯„ä¼°
                    test_reward = self.evaluate()
                    # ä¿å­˜æœ€ä½³æ¨¡å‹
                    if test_reward > self.best_reward:
                        self.best_reward = test_reward
                        self.save_checkpoint("best_model")
                        print(f"New best model with reward: {test_reward:.4f}")

                batch_count += 1
                global_step += 1

            avg_policy_loss = policy_loss_sum / max(1, batch_count)
            avg_reward = reward_sum / max(1, batch_count)

            print(f"Epoch {i+1}/{self.epoch} - Avg Policy Loss: {avg_policy_loss:.4f}, Avg Reward: {avg_reward:.4f}")
            
            # è®°å½•åˆ°wandb
            if self.use_wandb:
                wandb.log({
                    "epoch": i+1,
                    "avg_policy_loss": avg_policy_loss,
                    "avg_reward": avg_reward,
                })

            # åœ¨æµ‹è¯•é›†ä¸Šè¯„ä¼°
            test_reward = self.evaluate()
            

            self.save_checkpoint(f"checkpoint-epoch-{i+1}")
            
            # ä¿å­˜æœ€ä½³æ¨¡å‹
            if test_reward > self.best_reward:
                self.best_reward = test_reward
                self.save_checkpoint("best_model")
                print(f"New best model with reward: {test_reward:.4f}")

            
        wandb.finish()
        
    # def prepare_inputs(self,batch):
    #     """
    #     å¯¹on policyç”Ÿæˆçš„batché‡‡æ · + æ‰“åˆ† + è®¡ç®—ç›¸å¯¹ä¼˜åŠ¿
    #     """
    #     self.eval()
    #     for k, v in batch.items():
    #         if isinstance(v, torch.Tensor):
    #             batch[k] = v.to(self.device)
                
    #     pad_token_id = self.tokenizer.pad_token_id
        
    #     prompt=self.tokenizer.batch_decode(batch["input_ids"], skip_special_tokens=True)[0]
    #     input_ids=batch["input_ids"].repeat_interleave(self.num_generation, dim=0)
    #     with torch.no_grad():
            
    #         # sanitize_input_ids(self.tokenizer,batch["input_ids"],self.tokenizer.vocab_size)

    #         seq = self.policy_model.model.generate(
    #             batch["input_ids"],
    #             attention_mask=batch["attention_mask"],
    #             max_length=self.max_answer_seq_len,
    #             pad_token_id=self.tokenizer.pad_token_id,
    #             num_return_sequences=self.num_generation
    #             )

    #         seq_mask = seq.not_equal(pad_token_id).long()
    #         completions =self.get_completion(seq, input_ids, self.tokenizer)
            
        
    #         outputs=self.policy_model(seq, attention_mask=seq_mask)
    #         outputs_ref=self.ref_model(seq, attention_mask=seq_mask)
    #         rwd_score=self.compute_reward_score(seq,attention_mask=seq_mask,completions=completions,labels=batch["labels"])
    #         # print(f"rwd_score:{rwd_score}")
    #         adv=self.compute_adv(rwd_score)
    #         # print(f"adv:{adv}")
            

    #     logits = outputs.logits
    #     logits_ref = outputs_ref.logits

    #     prepared={
    #         'prompts': input_ids,
    #         'logprobs': gather_log_probs(logits[:, :-1, :], seq[:, 1:]),
    #         'ref_logprobs': gather_log_probs(logits_ref[:, :-1, :], seq[:,1:]),
    #         'rewards': rwd_score,
    #         'advantage':adv,
    #         'input_ids': seq,
    #         "attention_mask": seq_mask,
    #         "completions":completions,
    #     }


    #     return prepared


    def prepare_inputs(self,batch):
        """
        å¯¹on policyç”Ÿæˆçš„batché‡‡æ · + æ‰“åˆ† + è®¡ç®—ç›¸å¯¹ä¼˜åŠ¿
        """
        self.eval()
        for k, v in batch.items():
            if isinstance(v, torch.Tensor):
                batch[k] = v.to(self.device)
                
        pad_token_id = self.tokenizer.pad_token_id
        input_ids=batch["input_ids"]
        prompt=self.tokenizer.batch_decode(input_ids, skip_special_tokens=True)[0]

        with torch.no_grad():
            
            # sanitize_input_ids(self.tokenizer,batch["input_ids"],self.tokenizer.vocab_size)

            seq = self.policy_model.model.generate(
                batch["input_ids"],
                attention_mask=batch["attention_mask"],
                max_length=self.max_answer_seq_len,
                # num_return_sequences=self.num_generation,
                pad_token_id=self.tokenizer.pad_token_id,
                )

            seq_mask = seq.not_equal(pad_token_id).long()
            completions =self.get_completion(seq, input_ids, self.tokenizer)
            
        
            outputs=self.policy_model(seq, attention_mask=seq_mask)
            outputs_ref=self.ref_model(seq, attention_mask=seq_mask)

            logits = outputs.logits
            logits_ref = outputs_ref.logits

            rwd_score=self.compute_reward_score(seq,attention_mask=seq_mask,completions=completions,labels=batch["labels"])
            # print(f"rwd_score:{rwd_score}")
            rwd,_=self.compute_rwd(input_ids,logits,
                                              logits_ref, rwd_score,
                                               seq_mask)
            # print(f"rwd:{rwd}")
            adv,_=self.compute_grpo_outcome_advantage(rwd)
            # print(f"adv:{adv}")
            print(adv.shape)
            

        

        prepared={
            'prompts': input_ids,
            'logprobs': gather_log_probs(logits[:, :-1, :], seq[:, 1:]),
            'ref_logprobs': gather_log_probs(logits_ref[:, :-1, :], seq[:,1:]),
            'rewards': rwd_score,
            'advantage':adv,
            'input_ids': seq,
            "attention_mask": seq_mask,
            "completions":completions,
        }


        return prepared

    def get_completion(self,seq,input_ids, tokenizer):
        """
        ä»ç”Ÿæˆçš„åºåˆ—ä¸­æå– completion éƒ¨åˆ†ã€‚

        Args:
            seq (torch.Tensor): ç”Ÿæˆçš„åºåˆ—ï¼Œå½¢çŠ¶ä¸º (batch_size, seq_len)ã€‚
            input_ids (torch.Tensor): è¾“å…¥çš„ prompt çš„ token IDsï¼Œå½¢çŠ¶ä¸º (batch_size, prompt_len)ã€‚
            tokenizer: ç”¨äºè§£ç  token IDs çš„ tokenizerã€‚

        Returns:
            list: åŒ…å« completion å­—ç¬¦ä¸²çš„åˆ—è¡¨ã€‚
        """
        completions = []
        completions_mask=[]
        batch_size = seq.size(0)

        # åˆ›å»ºä¸€ä¸ªæ©ç å¼ é‡ï¼Œæ ‡è®°æ¯ä¸ªä½ç½®æ˜¯å¦ä¸ºcompletionéƒ¨åˆ†
        is_completion = torch.zeros_like(seq, dtype=torch.bool)
        
        # å¯¹äºæ¯ä¸ªæ ·æœ¬ï¼Œå°†prompt_lenä¹‹åçš„ä½ç½®æ ‡è®°ä¸ºTrue
        for i in range(batch_size):
            prompt_len = input_ids[i].size(0)
            if prompt_len < self.max_answer_seq_len:
                is_completion[i, prompt_len:] = True
        
        # åˆ›å»ºä¸€ä¸ªæ–°çš„tensoræ¥ä¿å­˜æ‰€æœ‰completionéƒ¨åˆ†çš„token IDs
        # å°†écompletionéƒ¨åˆ†æ›¿æ¢ä¸ºpad token
        completion_ids = seq.clone()
        completion_ids[~is_completion] = tokenizer.pad_token_id
        
        # ä½¿ç”¨batch_decodeä¸€æ¬¡æ€§è§£ç æ‰€æœ‰completion
        decoded = tokenizer.batch_decode(completion_ids, skip_special_tokens=True)
        
        # æ¸…ç†ç»“æœï¼ˆç§»é™¤å¯èƒ½å­˜åœ¨çš„paddingé€ æˆçš„å‰ç¼€ç©ºç™½ï¼‰
        completions = [text.strip() for text in decoded]
        return completions
    # def get_completion(self,seq,input_ids, tokenizer):
    #     """
    #     ä»ç”Ÿæˆçš„åºåˆ—ä¸­æå– completion éƒ¨åˆ†ã€‚

    #     Args:
    #         seq (torch.Tensor): ç”Ÿæˆçš„åºåˆ—ï¼Œå½¢çŠ¶ä¸º (batch_size, seq_len)ã€‚
    #         input_ids (torch.Tensor): è¾“å…¥çš„ prompt çš„ token IDsï¼Œå½¢çŠ¶ä¸º (batch_size, prompt_len)ã€‚
    #         tokenizer: ç”¨äºè§£ç  token IDs çš„ tokenizerã€‚

    #     Returns:
    #         list: åŒ…å« completion å­—ç¬¦ä¸²çš„åˆ—è¡¨ã€‚
    #     """
    #     completions = []
    #     completions_mask=[]
    #     batch_size = seq.size(0)

    #     for i in range(batch_size):
    #         # è·å–å½“å‰æ ·æœ¬çš„ prompt é•¿åº¦
    #         prompt_len = input_ids[i].size(0)
    #         # æå– completion éƒ¨åˆ†çš„ token IDs
    #         completion_ids = seq[i, prompt_len:]

    #         # è§£ç  completion éƒ¨åˆ†
    #         completion = tokenizer.decode(completion_ids, skip_special_tokens=True)
    #         completions.append(completion)

    #     # print(completions)
    #     return completions


    def step(self, prepared_inputs):
        """
        å•æ­¥æ›´æ–°
        """
        # train the rlhf mode here
        ### process the old outputs
        prompts = prepared_inputs['prompts']
        log_probs = prepared_inputs['logprobs']
        ref_log_probs = prepared_inputs['ref_logprobs']
        reward_score = prepared_inputs['rewards']
        attention_mask = prepared_inputs['attention_mask']
        seq = prepared_inputs['input_ids']
        adv=prepared_inputs['advantage']

        start = prompts.size()[-1] - 1
        # #å› ä¸ºç¬¬ä¸€ä¸ªtokenæ˜¯è¾“å…¥
        action_mask = attention_mask[:, 1:]
        # ç¡®ä¿åªæœ‰ç”Ÿæˆéƒ¨åˆ†çš„æœ‰æ•ˆ token å‚ä¸è®­ç»ƒï¼Œå¿½ç•¥ padding éƒ¨åˆ†ã€‚
        ends = start +attention_mask[:, start+1:].sum(1)-1


        batch = {'input_ids': seq, "attention_mask": attention_mask}

        policy_prob = self.policy_model(**batch).logits
        
            
            

        policy_log_prob = gather_log_probs(policy_prob[:, :-1, :], seq[:, 1:])
        print("policy_log_prob.requires_grad:", policy_log_prob.requires_grad)
        action_mask[:, 0:start]=0
        policy_loss = self.compute_loss(policy_log_prob[:, start:],
                                        log_probs[:, start:],adv[:, start+1:],
                                        action_mask[:, start:])
        policy_loss.backward()
        # # self.policy_model.backward(policy_loss)
        self.policy_optimizer.step()
        self.policy_lr_scheduler.step()

        self.policy_optimizer.zero_grad()

        return policy_loss
    
    def compute_reward_score(self,seq,attention_mask,completions,labels):
        """
        æ ¹æ®reward modelæˆ–è€…reward functionè®¡ç®—rwd
        å¥–åŠ±æ˜¯sentence-levelçš„ï¼Œå¥–åŠ±æ˜¯æ ‡é‡å€¼ã€‚
        """
        size=seq.shape[0]
        if self.reward=="model":
            with torch.no_grad():
                rwd_score=self.reward(seq,attention_mask=attention_mask)
            return rwd_score                
        elif self.reward_mode=="rule":
            if self.reward_fn=="gsm8k":
                rwd_score=[0 for i in range(size)]
                for rwd_fn in self.reward:
                    rewards=rwd_fn(completions,labels) 
                    rwd_score = [r1 + r2 for r1, r2 in zip(rwd_score, rewards)]
            elif self.reward_fn=="kk":
                rwd_score=[]
            
                for s, g in zip(seq,labels):
                    seq_text = self.tokenizer.decode(s, skip_special_tokens=True)

                    gt_text=self.tokenizer.decode(g, skip_special_tokens=True)
                    ground_truth=json.loads(gt_text)

                    score=compute_score(seq_text,ground_truth)
                    rwd_score.append([score])
            rwd_score=torch.tensor(rwd_score).to(device=self.device)
            return rwd_score
    
    
    def compute_rwd(self, prompts, log_probs, ref_log_probs, reward_score,action_mask):
        rwd=[]

        kl_divergence_estimate = -self.kl_ctl * (log_probs - ref_log_probs)
        rewards = kl_divergence_estimate
        start = prompts.shape[1] - 1
        ends = start +action_mask[:, start+1:].sum(1)-1
        reward_clip = torch.clamp(reward_score, -self.clip_reward_value,
                                  self.clip_reward_value)
        batch_size = log_probs.shape[0]
         # åœ¨Lç»´åº¦ä¸Šï¼Œç­”æ¡ˆéƒ¨åˆ†æ¯ä¸ªä½ç½®éƒ½æœ‰KLæ•£åº¦ï¼Œä½†æ˜¯åªåœ¨æœ€åä¸€ä¸ªä½ç½®åŠ ä¸Šå¥–åŠ±å€¼
        for j in range(batch_size):
            rewards[j, ends[j]] += reward_clip[j][-1]
            rwd.append(reward_clip[j][-1])
        return rewards,torch.tensor(rwd,dtype=torch.float16)

    def compute_adv(self,rewards):
        """
        æ ¹æ®reward scoreè®¡ç®—advantage
        åŒ…å«è¿‡ç¨‹ç›‘ç£å¼ºåŒ–å­¦ä¹ +GRPO å’Œ ç»“æœç›‘ç£å¼ºåŒ–å­¦ä¹ +GRPO ä¸¤ç§æ–¹å¼
        Ai,t=(ri-mean(r))/std(r), tå¯¹åº”token-levelä¼˜åŠ¿ï¼Œå³ä¸€ä¸ªå¥å­ä¸­ï¼Œæ¯ä¸ªtokenå¯¹åº”çš„ä¼˜åŠ¿æ˜¯ä¸€æ ·çš„ã€‚è¿™ç§æ–¹å¼çš„å¥½å¤„åœ¨äºï¼Œä¼°è®¡éƒ½æ˜¯ä»çœŸå®çš„ç¯å¢ƒrewardè®¡ç®—å¾—æ¥ï¼Œè€Œä¸æ˜¯é€šè¿‡ä»·å€¼ä¼°è®¡è®¡ç®—è€Œå¾—ã€‚
        """
        
        rewards = torch.tensor(rewards, dtype = torch.float).to(self.device)
        A = (rewards - rewards.mean()) / (rewards.std() + self.epsilon)
        return A


    def compute_grpo_outcome_advantage(self, token_level_rewards: torch.Tensor,
                                    epsilon: float = 1e-6):
        """
        Compute advantage for GRPO, operating only on Outcome reward 
        (with only one scalar reward for each response).
        Args:
            token_level_rewards: `(torch.Tensor)`
                shape: (bs, response_length)
            eos_mask: `(torch.Tensor)`
                shape: (bs, response_length)
        
        Returns:
            advantages: `(torch.Tensor)`
                shape: (bs, response_length)
            Returns: `(torch.Tensor)`
                shape: (bs, response_length)
        """
        response_length = token_level_rewards.shape[-1]
        non_zero_mask = (token_level_rewards != 0)
        scores = (token_level_rewards * non_zero_mask).sum(dim=-1)

        id2score = defaultdict(list)
        id2mean = {}
        id2std = {}

        # print(scores.shape)

        with torch.no_grad():
            bsz = scores.shape[0]
            for i in range(bsz):
                id2score[i].append(scores[i])
            for idx in id2score:
                if len(id2score[idx]) == 1:
                    id2mean[idx] = torch.tensor(0.0)
                    id2std[idx] = torch.tensor(1.0)
                elif len(id2score[idx]) > 1:
                    id2mean[idx] = torch.mean(torch.tensor(id2score[idx]))
                    id2std[idx] = torch.std(torch.tensor([id2score[idx]]))
                else:
                    raise ValueError(f"no score in prompt index: {idx}")
            for i in range(bsz):
                scores[i] = (scores[i] - id2mean[i]) / (id2std[i] + epsilon)
            # scores = scores.unsqueeze(-1).tile([1, response_length])
            # print(scores.shape)

        return scores, scores



    # def compute_loss(self,log_probs,old_log_probs,ref_log_probs,mask,adv):
    #     """
    #     æ ¹æ®advantageå’Œklæ•£åº¦è®¡ç®—loss
    #     GRPO KLæ˜¯token-levelçš„
    #     GRPOå¹¶æ²¡æœ‰åœ¨å¥–åŠ±ä¸­æ·»åŠ KLæƒ©ç½šï¼Œè€Œæ˜¯é€šè¿‡ç›´æ¥å°†è®­ç»ƒç­–ç•¥å’Œå‚è€ƒç­–ç•¥ä¹‹é—´çš„KLæ•£åº¦æ·»åŠ åˆ°æŸå¤±å‡½æ•°ä¸­æ¥è¿›è¡Œæ­£åˆ™åŒ–,ä»è€Œé¿å…äº†ä½¿å¾—ğ´^ğ‘–,ğ‘¡çš„è®¡ç®—å˜å¾—å¤æ‚
    #     å½“å‰ç­–ç•¥å¦‚æœå’Œrefç­–ç•¥æ¥è¿‘ï¼Œåˆ™klæ¥è¿‘0ï¼Œlosså¯èƒ½æ˜¯è´Ÿæ•°
    #     """
    #     len_oi=mask[:, ].sum(1)
        
    #     # kl
    #     # kl=ref_log_probs.exp() / log_probs.exp()- (ref_log_probs - log_probs) - 1
    #     ratio=torch.exp(log_probs - old_log_probs)
    #     print(ratio.shape)
    #     print(adv.shape)


    #     adv=adv.unsqueeze(dim = 1)  # [a, b ,c] -> [[a], [b], [c]]
    #     print(adv.shape)

    #     loss1=ratio*adv
    #     loss2=reward_clip = torch.clamp(ratio, 1.0 - self.cliprange,
    #                               1.0 + self.cliprange)*adv
    #     loss=(torch.minimum(loss1,loss2)-self.beta*kl)*mask
    #     loss=-(1/self.num_generation)*(1/len_oi.unsqueeze(dim = 1))*loss
    #     loss = loss.sum()

    #     return loss

    def compute_loss(self, logprobs, old_logprobs, advantages, mask):
        # é‡è¦æ€§é‡‡æ ·æƒé‡è®¡ç®— ratio = exp(log(new)-log(old)) å› ä¸ºppoæ˜¯off policyçš„ï¼Œæ‰€ä»¥éœ€è¦åŠ ä¸Šratio
        log_ratio = (logprobs - old_logprobs) * mask
        ratio = torch.exp(log_ratio)

        # print(ratio.shape)
        # print(advantages.shape)
        pg_loss1 = -advantages * ratio
        pg_loss2 = -advantages * torch.clamp(ratio, 1.0 - self.cliprange,
                                             1.0 + self.cliprange)
        pg_loss = torch.sum(torch.max(pg_loss1, pg_loss2) * mask) / mask.sum()
        return pg_loss

    





    def evaluate(self):
        """
        è¯„ä¼°å½“å‰policyçš„æ•ˆæœ
        """
        print("==============Evaluating on test set==============")
        self.policy_model.eval()
        total_reward = 0
        generated_examples = []
        num_samples = min(5, len(self.test_dataloader))  # ä»…è®°å½•å°‘é‡æ ·æœ¬ç”¨äºå±•ç¤º
        seq_length=0
        
        with torch.no_grad():
            for idx, batch in enumerate(tqdm(self.test_dataloader, desc="Evaluating")):
                # å°†æ•°æ®ç§»åˆ°ç›¸åº”è®¾å¤‡
                for k, v in batch.items():
                    if isinstance(v, torch.Tensor):
                        batch[k] = v.to(self.device)
                
                input_ids = batch["input_ids"]
                prompts = self.tokenizer.batch_decode(input_ids, skip_special_tokens=True)
                
                try:
                    seq = self.policy_model.model.generate(
                        batch["input_ids"],
                        attention_mask=batch["attention_mask"],
                        max_length=self.max_answer_seq_len,
                        pad_token_id=self.tokenizer.pad_token_id,
                    )
                    
                    seq_mask = seq.not_equal(self.tokenizer.pad_token_id).long()
                    
                    # è®¡ç®—å¥–åŠ±åˆ†æ•°
                    reward = self.compute_reward_score(seq, attention_mask=seq_mask,completions=None, labels=batch["labels"])
                    total_reward += reward.float().sum().item()
                    
                    # è§£ç ç”Ÿæˆçš„å›ç­”
                    generations = self.tokenizer.batch_decode(seq, skip_special_tokens=True)
                    seq_length+=seq.shape[1]


                    # ä¿å­˜ä¸€äº›æ ·æœ¬ç”¨äºå±•ç¤º
                    if idx < num_samples:
                        for p, g, r in zip(prompts, generations, reward):
                            generated_examples.append({
                                "prompt": p,
                                "generation": g,
                                "reward": r.item()
                            })
                
                except Exception as e:
                    print(f"Error during evaluation: {e}")
                    continue
        
        # è®¡ç®—å¹³å‡å¥–åŠ±
        avg_reward = total_reward / len(self.test_dataloader)
        seq_length=seq_length/len(self.test_dataloader)
        # avg_reward = total_reward / len(self.test_dataloader*self.batch_size)
        print(f"Test set - Average reward: {avg_reward:.4f}, test seq length: {seq_length}")
        

        wandb.log({"test_reward": avg_reward,"test_seq_length":seq_length})
        
        # åˆ›å»ºä¸€ä¸ªè¡¨æ ¼è®°å½•ç”Ÿæˆæ ·æœ¬
        if generated_examples:
            table = wandb.Table(columns=["prompt", "generation", "reward"])
            for example in generated_examples:
                table.add_data(example["prompt"], example["generation"], example["reward"])
            wandb.log({"generation_examples": table})
            wandb.log({"generation_examples": generated_examples})
        
        
        return avg_reward

    def save_checkpoint(self,checkpoint_name):
        """
        ä¿å­˜å½“å‰policy
        """
        checkpoint_dir = self.output_dir / checkpoint_name
        checkpoint_dir.mkdir(exist_ok=True, parents=True)
        
        # ä¿å­˜æ¨¡å‹
        self.policy_model.save_pretrained(checkpoint_dir / "policy_model")
        
        # ä¿å­˜tokenizer
        self.tokenizer.save_pretrained(checkpoint_dir)
        
        # ä¿å­˜è®­ç»ƒé…ç½®
        with open(checkpoint_dir / "training_args.json", 'w') as f:
            json.dump(self.__dict__, f, ensure_ascii=False, indent=4, default=str)
        
        print(f"Model checkpoint saved to {checkpoint_dir}")

    def eval(self):
        self.policy_model.eval()
        if self.reward_mode=="model":
            self.reward_model.eval()
        self.ref_model.eval()

    def stack_batches(self,batch):
        # # å°†æ•°æ®ç§»åˆ°ç›¸åº”è®¾å¤‡
        # for k, v in batch.items():
        #     if isinstance(v, torch.Tensor):
        #         batch[k] = v.to(self.device)
        stack_batch=batch
        for i in range(self.num_generation-1):
            stack_batch={key: torch.cat([stack_batch[key], batch[key]], dim=0) for key in batch}
        return stack_batch



def data_prepare(tokenizer,data_lst,device):
    question_lst=[data['prompt'][0]['content']for data in data_lst]

    gt_lst=[json.dumps(data["reward_model"]["ground_truth"])for data in data_lst]

    train_data = tokenizer.batch_encode_plus(question_lst, max_length=400, padding="longest", truncation=True,return_tensors='pt').to(device) 
    label_data = tokenizer.batch_encode_plus(gt_lst, max_length=400, padding="longest", truncation=True, return_tensors='pt').to(device) 

    train_data["labels"] = label_data["input_ids"]

    return train_data


class CustomDataset(Dataset):               
    def __init__(self, sample):
        super(CustomDataset, self).__init__()
        self.sample = sample

    def __getitem__(self, item):
        res = {k: v[item] for k, v in self.sample.items()}
        return res

    def __len__(self):
        return len(self.sample['input_ids'])



def gather_log_probs(logits, labels):
    """
    è·å¾—labelçš„å¯¹æ•°æ¦‚ç‡
    """
    log_probs = F.log_softmax(logits, dim=-1)
    log_probs_labels = log_probs.gather(dim=-1, index=labels.unsqueeze(-1))
    return log_probs_labels.squeeze(-1)


def set_seed(seed=42):
    random.seed(seed)  # Python å†…ç½®çš„éšæœºæ•°ç”Ÿæˆå™¨
    numpy.random.seed(seed)  # NumPy çš„éšæœºæ•°ç”Ÿæˆå™¨
    torch.manual_seed(seed)  # PyTorch çš„ CPU éšæœºç§å­
    torch.cuda.manual_seed(seed)  # PyTorch çš„ GPU éšæœºç§å­ï¼ˆå•å¡ï¼‰
    torch.cuda.manual_seed_all(seed)  # PyTorch çš„ GPU éšæœºç§å­ï¼ˆå¤šå¡ï¼‰
    torch.backends.cudnn.deterministic = True  # è®© cudnn ä»¥ç¡®å®šæ€§æ¨¡å¼è¿è¡Œ
    torch.backends.cudnn.benchmark = False  # å…³é—­ benchmarkï¼Œä¿è¯å¯å¤ç°æ€§




if __name__=="__main__":
    os.environ["WANDB_MODE"] = "offline"
    parser = argparse.ArgumentParser()


    set_seed(42)

    

    

    # Models
    parser.add_argument("--pretrain_path", type=str, default='Qwen/Qwen2.5-0.5B-Instruct')
    # Dataset
    parser.add_argument("--train_path",default='../Logic-RL/data/kk/instruct/3ppl/train.parquet')
    parser.add_argument("--test_path", default='../Logic-RL/data/kk/instruct/3ppl/test.parquet')
    #wandb
    parser.add_argument("--use_wandb", default=True)
    #outputs
    parser.add_argument("--output_dir", default='outputs/kk/')
    parser.add_argument("--reward_model", default=None)


    args=parser.parse_args()

    device = torch.device("cuda" if torch.cuda.is_available() else "cpu")
    # device="cpu"
    print(f"device:{device}")

    args.device=device
    args.reward_fn=[format_reward,correctness_reward]

    grpoTrainer=GRPOTrainer(args)
    grpoTrainer.learn()