{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/wsy/anaconda3/envs/zero/lib/python3.9/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "# !pip install tf-keras # for some reason, Hugging Face cannot work without it\n",
    "# !pip install flash-attn # FlashAttention2\n",
    "# !pip install wandb # Weights and Biases\n",
    "# !pip install 'accelerate>=0.26.0'\n",
    "# !pip install transformers # Hugging Face Transformers API\n",
    "# !pip install datasets # Hugging Face Datasets API\n",
    "\n",
    "# Import necessary libraries\n",
    "# Basic Python libraries for various operations\n",
    "import random\n",
    "import copy\n",
    "import re\n",
    "import os\n",
    "import numpy as np\n",
    "import wandb\n",
    "\n",
    "# PyTorch and related libraries for deep learning\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.nn.utils.rnn import pad_sequence\n",
    "\n",
    "# Hugging Face libraries for transformer models\n",
    "from transformers import AutoModelForCausalLM, AutoTokenizer\n",
    "from datasets import load_dataset\n",
    "\n",
    "def set_random_seed(seed: int = 42):\n",
    "    \"\"\"\n",
    "    Set the random seed for reproducibility across Python, NumPy, and PyTorch.\n",
    "\n",
    "    Args:\n",
    "        seed (int): The seed value to use for random number generation.\n",
    "\n",
    "    Returns:\n",
    "        None\n",
    "\n",
    "    Explanation:\n",
    "        1. Sets seed for Python's built-in random module for basic random operations.\n",
    "        2. Sets seed for NumPy, ensuring consistent random number generation in array operations.\n",
    "        3. Sets seed for PyTorch CPU operations.\n",
    "        4. If CUDA is available, sets seed for all GPU devices.\n",
    "        5. Configures cuDNN to ensure deterministic behavior:\n",
    "           - Sets deterministic flag to True, ensuring reproducible results.\n",
    "           - Disables benchmarking to prevent algorithm selection based on hardware.\n",
    "\n",
    "    Note:\n",
    "        Setting deterministic behavior may impact performance but ensures consistent results\n",
    "        across multiple runs, which is crucial for debugging and research.\n",
    "    \"\"\"\n",
    "    # Set the seed for Python's built-in random module\n",
    "    random.seed(seed)\n",
    "    # Set the seed for NumPy\n",
    "    np.random.seed(seed)\n",
    "    # Set the seed for PyTorch\n",
    "    torch.manual_seed(seed)\n",
    "    if torch.cuda.is_available():\n",
    "        torch.cuda.manual_seed_all(seed)\n",
    "    # Ensure deterministic behavior in cuDNN (may impact performance)\n",
    "    torch.backends.cudnn.deterministic = True\n",
    "    torch.backends.cudnn.benchmark = False\n",
    "\n",
    "# Call the function to set random seed for reproducibility\n",
    "set_random_seed(42)\n",
    "\n",
    "# Set environment variables for Weights & Biases (wandb) logging\n",
    "os.environ[\"WANDB_API_KEY\"] = \"a03c41d31f8b3661b148b577f6f06348387d54fb\"\n",
    "os.environ[\"WANDB_PROJECT\"] = \"GRPO-Qwen-1.5-Instruct-Multi-GPU\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 4: Evaluation Functions\n",
    "\n",
    "Evaluation is crucial to track the model's progress. In this part, we define functions that allow us to evaluate the model on a set of examples. The evaluation functions perform the following tasks:\n",
    "\n",
    "- **Tokenize the prompt and generate a response:** The model's output is generated given the tokenized prompt.\n",
    "- **Extract the predicted answer:** The answer is extracted from the generated response.\n",
    "- **Compare the predicted answer with the expected answer:** This comparison is done using exact matching as well as numeric equivalence checks.\n",
    "\n",
    "Two helper functions, `_extract_last_number` and `_extract_single_number`, are used to extract numbers from text. The main evaluation function, `evaluate_model`, uses these helpers to determine if the predicted answer is correct:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "SYSTEM_PROMPT = \"\"\"\n",
    "Respond in the following format:\n",
    "<reasoning>\n",
    "...\n",
    "</reasoning>\n",
    "<answer>\n",
    "...\n",
    "</answer>\n",
    "\"\"\"\n",
    "\n",
    "def extract_answer_from_model_output(text):\n",
    "   \"\"\"\n",
    "   Extracts the value from the last <answer> tag in the text.\n",
    "\n",
    "   Args:\n",
    "       text (str): The model-generated text containing XML-style <answer> tags.\n",
    "\n",
    "   Returns:\n",
    "       str or None: The content inside the <answer> tags, or None if no valid answer is found.\n",
    "\n",
    "   Explanation:\n",
    "       1. Splits the text on the <answer> tag to isolate content after the tag.\n",
    "       2. Checks if at least one <answer> tag exists in the text.\n",
    "       3. For the last <answer> segment:\n",
    "          - Verifies it contains a closing </answer> tag.\n",
    "          - Extracts only the content between the tags.\n",
    "       4. Returns None if the answer is empty (just \"...\") or if tags are missing.\n",
    "   \"\"\"\n",
    "   # Split on <answer> and take everything after the last occurrence\n",
    "   parts = text.split(\"<answer>\")\n",
    "   if len(parts) < 2:  # No <answer> tag found\n",
    "       return None\n",
    "   last_part = parts[-1]\n",
    "\n",
    "   # Extract content up to </answer>\n",
    "   if \"</answer>\" not in last_part:\n",
    "       return None\n",
    "   answer = last_part.split(\"</answer>\")[0].strip()\n",
    "   return None if answer == \"...\" else answer\n",
    "\n",
    "\n",
    "def extract_last_number(text):\n",
    "   \"\"\"\n",
    "   Extracts the last number appearing in the text.\n",
    "\n",
    "   Args:\n",
    "       text (str): The text to extract a number from.\n",
    "\n",
    "   Returns:\n",
    "       float or None: The last number in the text, or None if no number is found.\n",
    "\n",
    "   Explanation:\n",
    "       1. Removes dollar signs and percent symbols from the text.\n",
    "       2. Uses regex to find a number that appears at the end of the text (possibly after whitespace).\n",
    "       3. The pattern matches numbers that appear at the end of the string, with or without decimal points.\n",
    "       4. Returns the found number as a float, or None if no match is found.\n",
    "   \"\"\"\n",
    "   text = text.replace('$', '').replace('%', '')\n",
    "   pattern = r'(?:^|\\s|=)\\s*(-?\\d*\\.?\\d+)\\s*$'\n",
    "   match = re.search(pattern, text)\n",
    "   return float(match.group(1)) if match else None\n",
    "\n",
    "def extract_single_number(text):\n",
    "   \"\"\"\n",
    "   Extracts a single number from text if exactly one number is present.\n",
    "\n",
    "   Args:\n",
    "       text (str): The text to extract a number from.\n",
    "\n",
    "   Returns:\n",
    "       float or None: The single number in the text, or None if zero or multiple numbers are found.\n",
    "\n",
    "   Explanation:\n",
    "       1. Uses regex to find all numbers in the text (including negative numbers and decimals).\n",
    "       2. If exactly one number is found, returns it as a float.\n",
    "       3. If zero or multiple numbers are found, returns None.\n",
    "   \"\"\"\n",
    "   numbers = re.findall(r'-?\\d*\\.?\\d+', text)\n",
    "   return float(numbers[0]) if len(numbers) == 1 else None\n",
    "\n",
    "def evaluate_model(model, tokenizer, eval_examples, device):\n",
    "   \"\"\"\n",
    "   Evaluates the model on a set of examples and prints detailed results.\n",
    "\n",
    "   Args:\n",
    "       model: The language model to evaluate.\n",
    "       tokenizer: The tokenizer for encoding inputs and decoding outputs.\n",
    "       eval_examples (list): List of evaluation examples, each containing \"prompt\" and \"answer\".\n",
    "       device: The device (CPU or GPU) to run evaluation on.\n",
    "\n",
    "   Returns:\n",
    "       float: The accuracy percentage (correct predictions / total examples * 100).\n",
    "\n",
    "   Explanation:\n",
    "       1. Sets the model to evaluation mode.\n",
    "       2. For each example in the evaluation set:\n",
    "          - Encodes the prompt and generates a response using the model.\n",
    "          - Extracts the predicted answer from the generated response.\n",
    "          - Compares the predicted answer with the expected answer using multiple methods:\n",
    "            a. Exact string matching\n",
    "            b. Single number extraction and comparison\n",
    "            c. Last number extraction and comparison\n",
    "          - Prints detailed information about each example.\n",
    "       3. Calculates and returns the overall accuracy.\n",
    "       4. Returns the model to training mode.\n",
    "   \"\"\"\n",
    "   model.eval()\n",
    "   correct = 0\n",
    "   total = len(eval_examples)\n",
    "   print(\"\\n\" + \"=\"*50)\n",
    "   print(\"EVALUATION ON\", total, \"EXAMPLES\")\n",
    "   print(\"=\"*50)\n",
    "\n",
    "   for example in eval_examples:\n",
    "       # Get the prompt and expected answer\n",
    "       full_prompt = example[\"prompt\"]\n",
    "       expected = example[\"answer\"]\n",
    "\n",
    "       # Tokenize and generate response\n",
    "       inputs = tokenizer.encode(full_prompt, return_tensors=\"pt\").to(device)\n",
    "       with torch.no_grad():\n",
    "           outputs = model.generate(\n",
    "               inputs,\n",
    "               max_new_tokens=512,\n",
    "               temperature=0.7,\n",
    "               num_return_sequences=1,\n",
    "               pad_token_id=tokenizer.pad_token_id,\n",
    "               eos_token_id=tokenizer.eos_token_id,\n",
    "               forced_eos_token_id=tokenizer.eos_token_id,\n",
    "               early_stopping=False,\n",
    "           )\n",
    "       response = tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
    "\n",
    "       try:\n",
    "           # Extract answer and check correctness\n",
    "           predicted = extract_answer_from_model_output(response)\n",
    "\n",
    "           # Try different matching methods\n",
    "           if predicted == expected:  # Exact match\n",
    "               is_correct = True\n",
    "           else:\n",
    "               # Try single number matching\n",
    "               pred_num = extract_single_number(str(predicted))\n",
    "               exp_num = extract_single_number(str(expected))\n",
    "               if pred_num is not None and exp_num is not None and pred_num == exp_num:\n",
    "                   is_correct = True\n",
    "               else:\n",
    "                   # Try last number matching\n",
    "                   pred_num = extract_last_number(str(predicted))\n",
    "                   exp_num = extract_last_number(str(expected))\n",
    "                   is_correct = (pred_num is not None and exp_num is not None and\n",
    "                               pred_num == exp_num)\n",
    "\n",
    "           # Update counter for correct answers\n",
    "           if is_correct:\n",
    "               correct += 1\n",
    "\n",
    "           # Print evaluation details\n",
    "           print(\"\\nPrompt:\")\n",
    "           print(full_prompt)\n",
    "           print(\"\\nExpected Answer:\")\n",
    "           print(expected)\n",
    "           print(\"\\nExtracted Answer:\")\n",
    "           print(predicted)\n",
    "           print(\"\\nFull Generated Response:\")\n",
    "           print(response)\n",
    "           print(\"\\nCorrect:\", \"✓\" if is_correct else \"✗\")\n",
    "           print(\"-\"*50)\n",
    "\n",
    "       except Exception as e:\n",
    "           print(\"\\nFailed to parse model output for prompt:\")\n",
    "           print(full_prompt)\n",
    "           print(\"Error:\", e)\n",
    "           print(\"-\"*50)\n",
    "\n",
    "   # Calculate and print final accuracy\n",
    "   accuracy = (correct / total) * 100\n",
    "   print(f\"\\nAccuracy: {accuracy:.2f}% ({correct}/{total})\")\n",
    "   print(\"=\"*50)\n",
    "\n",
    "   # Return model to training mode\n",
    "   model.train()\n",
    "   return accuracy"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "zero",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
